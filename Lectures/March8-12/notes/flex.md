# Lex/yacc Lecture

## lex/yacc sequence

* To use lex/yacc to produce a program to analyze source code in 
  Language L, the steps are as follows
  
  * Create a file, L.lex, this contains L's tokenization rules
  * Create a file, L.yacc, containing L's parsing rules
  * Run lex on L.lex to produce lex.yy.c
  * Run yacc on L.yacc to produce y.tab.c
  * Compile the two .c files to produce our tool
  * gcc y.tab.c lex.yy.c -o analyzeL

* We can now use the tool on source code written in L

```
./analyzeL < someLsourceCode
```

## Lex file organization

> A lex file is divided into three core sections, seperated by '%%'

* The first section is used to define useful regex character sets,
  C includes, prototypes, global vars, and typedefs
  
* The second section is used for the actual token rules and code to be
  applied when a token is identified
  
* The final section contains regular C code, typically the implementations
  for any functions prototyped in the first section
  
* SYNTAX GOTCHAS: when using C style comments in the lex file, don't start them
  on the first character of the line, put a space first
  
## Notes on the C files/Functions

* y.tab.h will be auto-generated by lex

* yywrap is automatically called at the end of processing and is generally
  used to clean up any dynamically-allocated memory and call any summary routines
  we want to run
  
* yywrap must return 1 if processing completes normally

* yyerror is called in the event of tokenizing errors, and gets passed a string
  representing the error information -- typically it displays the string and returns 1
  
## Token rules section

* This section is after the first %%, and contains rules describing each token

* Each token rule has a regular expression defining what makes uip a valid token of that type
  plus a block of C code (in {}) dictating what to do with that token type
  
* The block of C code associated with the token must return a constant identifying that token,
  either a character (e.g 'x') or a token named in our .yacc file e.g STRING
  
* The yacc and lex files end up depending on eachother

## Toke run ordering

* Rules are processed in the order they appear in the .lex file, and the first mathcing rule is applied

* If tokens overlap we want to be careful about our rule order

* Suppose we have a keyword named 'while', and our identifiers are any non-empty string of alphabetic
  characters
  
* The lex file needs to check for the keywords first, so it doesnt accidentally treat 'while'
  as an identifier
  
## Working with token information

* For our identifier, if we were building up a symbol table of information
  we want to know more than just "we saw an identifier", what was the actual identifier
  (x, foo, ..?), perhaps what scope we were in when we saw it, etc
  
* Within the code section for the token, a variable named yytext is available, and holds
  the actual string for the token (e.g. "x", "foo", "==", "99.4", etc)
  
* Another variable, named yylval, is available to store things we want to remember about that
  specific token, to use later (the data type for yylval for each token type gets specified
  in the .yacc file)
  
## Example of yytext and yylval

``` c
/*
  if a DIGIT is recognized as 17, yytext will have the value "17/0",
  so we can save the length of the variable in col, and set a member the yylval
  struct "num" to hold the converted integer representation. The data type that we
  store them in is defined in the yacc file, and we will use them through the accessor
  yylval.
*/

({DIGIT})+  { col+=strlen(yytext); yylval.num=atoi(yytext); return(INT) }
```

## More on code blocks and yylval

* For each token type, the .yacc file can specify any C data type for its yylval
  yylval: ints, chars, structs etc
  
* In the lex code block for the token type we need to make sure we treat the yylval as the
  matching type
  
* Our .lex code block can also make use of any of the global variables we declared in the top
  declarations section, updating them based on the current token type and content of yytext.
